{"cells":[{"cell_type":"markdown","source":["# **TD Detecting Higgs bosons**\n","\n","##Purpose of the Lab and Methodological Overview\n","\n","The purpose of this laboratory session is to apply and critically evaluate **advanced machine learning paradigms** on the challenging problem of Higgs Boson particle detection—a classification task characterized by **high dimensionality, noise, and complex feature interactions**. Students are guided through a structured experimental workflow designed to progressively incorporate increasingly sophisticated learning strategies.\n","\n","In the first stage, students perform **data preparation and preprocessing**, addressing issues such as normalization, handling missing values, and feature extraction to ensure data suitability for machine learning. In the second stage, they implement baseline classifiers—including Decision Trees, Logistic Regression, and Bayesian models—to establish reference performance metrics without additional learning paradigms. The third stage introduces e**nsemble learning methods** (Bagging and Boosting), emphasizing how the aggregation of weak learners can enhance predictive stability and reduce variance. In the fourth stage, students integrate the **active learning paradigm** using the **modAL** Python library, enabling the model to iteratively query the most informative samples to reduce labeling effort while improving accuracy. Finally, in the fifth stage, they develop a **hybrid approach that combines ensemble and active learning**, exploring how sampling-based paradigms can synergize to handle the Higgs dataset’s inherent complexity.\n","\n","Through this progressive design, the lab aims to foster a deep understanding of sampling-based paradigms as robust strategies for learning from difficult, imbalanced, or noisy data—illustrating their capacity to improve model generalization and efficiency in high-stakes scientific contexts such as particle physics."],"metadata":{"id":"6uG3fEkthdr_"}},{"cell_type":"markdown","metadata":{"id":"V4ZbiHvjiBvb"},"source":["## I- Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"slHBuzRTiBvg"},"source":["### 1- Importing libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSmZESWviBvh"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import csv\n","import time\n","import math\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.impute import SimpleImputer\n","from sklearn import model_selection\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l4Vaii2Nia7y","outputId":"5f440a79-9dec-48e6-b767-a5ed342699dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount ('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"0O35qZ21iBvi"},"source":["### 2- Reading CSV files\n","Data set available in http://opendata.cern.ch/record/328\n","\n","**If the dataset is very large, you can select a sub-sample to work with.**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzzwW-mGiBvj"},"outputs":[],"source":["\n","dataFilename = '/content/drive/MyDrive/Data/atlas-higgs.csv'\n","data_complet = pd.read_csv(dataFilename)\n","#select only a sample\n","data = data_complet.sample(frac=0.1, random_state=42)\n","print(data.shape)\n","print(data.dtypes)\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"i6tMg1EliBvk"},"source":["### 3- Delete superfluous columns\n","Use the `del` function to delete the 'EventId', 'Weight', 'KaggleSet', 'KaggleWeight' columns, then transform the 'label' column into 0/1 binary values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfAOM1bGiBvk"},"outputs":[],"source":["del(data['EventId'])\n","#............."]},{"cell_type":"markdown","source":["### 4- Check Outliers\n","Check the dataset for outliers by displaying the boxplots of all columns."],"metadata":{"id":"7kvhRnImacJE"}},{"cell_type":"code","source":["#boxplot display for different attributes"],"metadata":{"id":"Ka2Cq4XaLqx2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GpktkCNRrLzG"},"source":["## 5- Check Unbalance Ratio\n","Display the histogram of the target column to check the existence of a disequilibrium between the positive and negative classes."]},{"cell_type":"code","source":[],"metadata":{"id":"CyauJ4kleRa3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AZhTRTXDiBvo"},"source":["### 6- Split data to Train and test\n","Separate *input* data from *target* and split the dataset into train and test (30% for test)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lm6aTJCniBvp"},"outputs":[],"source":["#sperate the output\n","\n","#subdivision of test sample data = 30%.\n","\n","\n","print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"P9GQUNBNiBvm"},"source":["### 7 - Dealing with missing data\n","Missing data in this file are designated -999. To replace them, use the `SimpleImputer` class."]},{"cell_type":"code","source":["#Handling missing data designated by -999\n"],"metadata":{"id":"p6WLuAGSK_Xl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BuIbYwftiBvn"},"source":["### 8-  Data calibration with *MinMaxScaler* from Sci-kit Learn\n","Scale all columns with the \"MinMaxScaler\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvsxMUW9iBvn"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","#...\n"]},{"cell_type":"markdown","source":["### 9- Apply a dimensionality reduction technique\n","Apply the PCA method to reduce training data size while retaining 95% variance."],"metadata":{"id":"95O3swWlP-En"}},{"cell_type":"code","source":["#import PCA class\n","\n","#Apply decomposition with 95% variance\n","\n","#Transforming learning data\n","....."],"metadata":{"id":"cmCgtvkDP9VS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 10 - Create a preprocessing pipleline\n","Add all the previouos preprocessing steps to a pipeline to be reused for the test data."],"metadata":{"id":"sZgwt5iTfJDS"}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\n","\n","# Create a pipeline with the preprocessing steps\n","preprocessing_pipeline = Pipeline([...."],"metadata":{"id":"sWel8bcHfeZt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CECMSyYbiBvp"},"source":["# II- Learning I: Models Only (No additional Paradigms)"]},{"cell_type":"markdown","source":["Creation of a list of performance measures for model comparison"],"metadata":{"id":"s0Q8E7XResxS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxzDckA95VTp"},"outputs":[],"source":["#for saving results\n","list_accuracies=[]\n","list_F1=[]\n","list_times=[]"]},{"cell_type":"markdown","metadata":{"id":"rmObvgVkiBvq"},"source":["## 1- Decision Tree"]},{"cell_type":"code","source":["#first preprocess the test set\n"],"metadata":{"id":"vnr8Knl4XtVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVrP2l1LiBvq"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","#create an instance of a DecisionTree classifier\n","dt_class= .....\n","deb = time.time()\n","#If training fails, rerun the code\n","dt_class.fit(x_train, y_train)\n","fin=time.time()\n","res_dt = dt_class.predict(x_test)\n","print(\"temp\",(fin-deb))\n","list_times.append(fin-deb)\n","acc=accuracy_score(res_dt,y_test)\n","F1=f1_score(res_dt,y_test)\n","print(\"accuray : \",acc)\n","print(\"F1 Score : \", F1)\n","list_accuracies.append(acc)\n","list_F1.append(F1)\n","\n","classes=['Signal','Background']\n","cm = confusion_matrix(y_test, res_dt, labels=dt_class.classes_)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=dt_class.classes_)\n","disp.plot()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OYTqL9rbiBvq"},"source":["## 2- Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7Ur-A4_iBvr"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","lg_clf = LogisticRegression(solver='sag')\n","#The default solver for logistic regression is \"liblinear\", suitable for small and medium-sized problems\n","#sag\": is a solver based on stochastic optimization (stochastic mean gradient method).\n","#It is suitable for large problems.\n","deb = time.time()\n","#...."]},{"cell_type":"markdown","metadata":{"id":"6Ke2GgIFiBvr"},"source":["##  3- Baysien"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_LcoV9viBvr"},"outputs":[],"source":["from sklearn.naive_bayes import GaussianNB\n","gn_clf=GaussianNB()\n","deb = time.time()\n","#......"]},{"cell_type":"markdown","metadata":{"id":"C_lm1iMFiBvs"},"source":["### Plotting results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTXeMKXHiBvs"},"outputs":[],"source":["x = ['DT', 'LR', 'Gauss']\n","plt.grid()\n","plt.plot(x,list_accuracies)\n","plt.plot(x,list_F1,'r')\n","#ajouter une légende\n","#....\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZwzS0vZ5AMw"},"outputs":[],"source":["#figures des temps de calcul\n","plt.plot(x,list_times, label='Execution Time')\n","#ajouter une légende\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"J9DyQBBQiBvs"},"source":["## III- Learning II: Ensemble Learning"]},{"cell_type":"markdown","metadata":{"id":"Xbf_cQ3CiBvs"},"source":["## III-1 Bagging"]},{"cell_type":"markdown","metadata":{"id":"MjdkS8kfiBvs"},"source":["### Random Forest\n","class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_XOEugPviBvs"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","deb = time.time()\n","...."]},{"cell_type":"markdown","metadata":{"id":"hvwaD8-TiBvt"},"source":["### Bagging classifier with Logistic Regression\n","class sklearn.ensemble.BaggingClassifier(estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0, base_estimator='deprecated')\n","\n","*   max_samples: maximum sample size relative to the initial sample\n","*   max_features: number of attributes to select randomly\n","*   bootstrap: Whether samples are drawn with replacement. If False, sampling is performed without replacement.\n","*   bootstrap_features: Whether features are drawn with replacement.\n","*   n_jobs: The number of jobs to run in parallel for fitting and predicting. -1 uses all available processors."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GX84hhjniBvt"},"outputs":[],"source":["from sklearn.ensemble import BaggingClassifier\n","#start with max_sample=1 and max_features=1\n"]},{"cell_type":"markdown","metadata":{"id":"TUpxizuiiBvu"},"source":["### Plotting results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PrcTKKhriBvu"},"outputs":[],"source":["x = ['DT', 'LR', 'Gauss', \"RF\", \"Bag LR\"]\n","plt.grid()\n","#plt.plot(...)\n","#..........\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"k5pqwfXJOO4q"},"outputs":[],"source":["#figures des temps de calcul\n","plt.grid()\n","plt.plot(x,list_times)"]},{"cell_type":"markdown","source":["### Disucussion\n","\n",".................."],"metadata":{"id":"ZFD4gsSagUp-"}},{"cell_type":"markdown","metadata":{"id":"rWHnzr6YBOrH"},"source":["#Exercice: #\n","Vary the values of the 'max_samples' and 'max_features' parameter to visualize the effect of sampling in ensemble learning on classifier quality.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GeN6-QuEB5BC"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_M2VVY-0iBvu"},"source":["## III - 2- Boosting"]},{"cell_type":"markdown","metadata":{"id":"ObjFMt4DiBvv"},"source":["### AdaBoost\n","lass sklearn.ensemble.AdaBoostClassifier(estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')\n","\n","An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZgiLen1HiBvv"},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier\n","deb = time.time()\n","...."]},{"cell_type":"markdown","metadata":{"id":"tGm2KawViBvv"},"source":["## Gradient Boost (Gradient Boosted Decision Trees)\n","\n","This algorithm builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the loss function, e.g. binary or multiclass log loss. Binary classification is a special case where only a single regression tree is induced.\n","\n","class sklearn.ensemble.GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CCQxp_-jiBvv"},"outputs":[],"source":["\n","from sklearn.ensemble import GradientBoostingClassifier\n"]},{"cell_type":"markdown","source":["### Plotting new results"],"metadata":{"id":"6rXACTUdnYJg"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TA-TQ1heV2Qi"},"outputs":[],"source":["x = ['DT', 'LR', 'Gauss', \"RF\", \"Bag LR\", \"AdaBoost\", \"GdBoost\" ]\n","plt.grid()\n","plt.plot(x,list_accuracies)\n","plt.plot(x,list_F1,'r')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Rlsb76OwiBvx"},"source":["# IV  Unbalance Learning (optional)"]},{"cell_type":"markdown","metadata":{"id":"B2TRRBvjtbkB"},"source":["### Over Sampling :\n","\n","Add at least one over sampling technique to the preporecssing pipeline and re-test the different models\n","*   Over  Samplig (OS)\n","*   SMOTE (synthetic sampling)\n","*   BSMOTE (Borderline Sampling)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Hj92vGguHuM"},"outputs":[],"source":["from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE\n","\n","#Over Sampling\n","\n","#SMOTE or BSMOTE # heavy in execution!!!!\n","sm = SMOTE(random_state=1)\n","...."]},{"cell_type":"markdown","metadata":{"id":"LF5MjQN0wGZK"},"source":["## New results after balancing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfwBeZvUiBvy"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# V Active Learning Integration\n","\n","Use modAL to actively select the most informative samples from the training pool.\n","\n","**Steps**:\n","\n","*   Start with a small labeled subset (e.g., 1% of training data)\n","*   Use uncertainty sampling to query new samples\n","*   Train a classifier (e.g. LogisticRegression)\n","*   Track performance after each query round"],"metadata":{"id":"osKx_ZZYtI8R"}},{"cell_type":"code","source":["#Install modAL\n","!pip install modAL-python"],"metadata":{"id":"LgA6k-VXtg4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import librairies\n","from modAL.models import ActiveLearner\n","from modAL.uncertainty import uncertainty_sampling\n","import warnings\n","warnings.filterwarnings(\n","    \"ignore\",\n","    category=FutureWarning,\n","    module='sklearn'\n",")"],"metadata":{"id":"NjQDHIystn9G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#step1-split an initial training set from the pool -training set)\n","\n","#step2 - Initialize the Active Learner with LogisticRegresion and UncertaintySampling\n","\n","#Step3- Active Learning loop\n","#record accuracy after each teaching (training) step\n","\n","#Step4: Plot the evolution of accuracy\n","\n"],"metadata":{"id":"gMcPCEVEurEj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# VI - Hybrid Active Learning + Ensemble\n","\n","Propose an hybrid classification model using modAL and RandomForestClassifier"],"metadata":{"id":"MCm2u0c3vm-_"}},{"cell_type":"code","source":[],"metadata":{"id":"r7XDnapcvmwP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#VII - Conclusion and Discussion"],"metadata":{"id":"rNP7wHZ6v9Fj"}},{"cell_type":"markdown","source":[],"metadata":{"id":"XCEHcPRwxQgB"}}],"metadata":{"anaconda-cloud":{},"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"nbformat":4,"nbformat_minor":0}